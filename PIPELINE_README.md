# Pipeline de Limpieza de Datos

Este documento describe el pipeline de limpieza de datos implementado basado en el notebook `Limpieza.ipynb`.

## üìã Descripci√≥n General

El pipeline procesa archivos Excel con datos hospitalarios crudos y genera datasets limpios por complejidad, listos para entrenamiento de modelos de predicci√≥n.

## üèóÔ∏è Arquitectura

```
backend/src/
‚îú‚îÄ‚îÄ pipeline/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ data_cleaner.py           # Limpieza inicial del Excel
‚îÇ   ‚îî‚îÄ‚îÄ prediction_preparer.py    # Preparaci√≥n de datos para predicci√≥n
‚îú‚îÄ‚îÄ routes/
‚îÇ   ‚îú‚îÄ‚îÄ data.py                   # Endpoints para datos semanales (EXISTENTES)
‚îÇ   ‚îî‚îÄ‚îÄ pipeline.py               # Endpoints para procesamiento de Excel (NUEVOS)
‚îî‚îÄ‚îÄ utils/
    ‚îî‚îÄ‚îÄ storage.py                # Gesti√≥n de almacenamiento (local/S3)
```

## üîå Nuevos Endpoints

### 1. `POST /pipeline/process-excel`

Procesa un archivo Excel crudo y genera CSVs por complejidad.

**Entrada:**

- Archivo Excel (.xlsx o .xls) con formato espec√≠fico

**Requisitos del Excel:**

- **M√≠nimo 3 hojas**
- **Hoja 0**: Datos de pacientes
  - `Servicio Ingreso (C√≥digo)`
  - `Fecha Ingreso Completa`
  - `Estancia (D√≠as)`
  - `Tipo de Paciente`
  - `Tipo de Ingreso`
- **Hoja 2**: Datos de servicios
  - `UO trat.`
  - `Complejidad`

**Proceso:**

1. Limpieza y normalizaci√≥n
2. Merge de hojas
3. Features temporales (semana, mes, estaci√≥n)
4. One-hot encoding
5. Agrupaci√≥n semanal
6. Creaci√≥n de lags (1, 2, 3, 4, 10, 52 semanas)
7. Generaci√≥n de CSVs

**Salida:**

```json
{
  "message": "Archivo procesado exitosamente",
  "complejidades_procesadas": {
    "Alta": "Procesada exitosamente",
    "Baja": "Procesada exitosamente",
    "Media": "Procesada exitosamente",
    "Neonatolog√≠a": "Datos insuficientes",
    "Pediatr√≠a": "Procesada exitosamente"
  },
  "archivos_generados": {
    "Alta": "./data/Alta.csv",
    "Baja": "./data/Baja.csv",
    "Media": "./data/Media.csv",
    "Pediatr√≠a": "./data/Pediatr√≠a.csv"
  },
  "estadisticas": {
    "Alta": {"filas": 120, "columnas": 18},
    "Baja": {"filas": 98, "columnas": 18}
  },
  "timestamp": "2025-10-26T12:30:45"
}
```

### 2. `GET /pipeline/download/{complejidad}`

Descarga el CSV procesado de una complejidad espec√≠fica.

**Par√°metros:**

- `complejidad`: Alta | Media | Baja | Neonatolog√≠a | Pediatr√≠a

**Salida:**

- Archivo CSV descargable

### 3. `GET /pipeline/download-all`

Descarga un ZIP con todos los CSVs procesados.

**Salida:**

- Archivo ZIP con CSVs de todas las complejidades procesadas

### 4. `GET /pipeline/status`

Obtiene el estado del pipeline y archivos disponibles.

**Salida:**

```json
{
  "message": "Estado del pipeline",
  "total_complejidades": 5,
  "archivos_procesados": 4,
  "storage_type": "local",
  "complejidades": {
    "Alta": {
      "procesado": true,
      "filas": 120,
      "columnas": 18,
      "ultima_semana": "2024-52"
    },
    "Media": {
      "procesado": true,
      "filas": 98,
      "columnas": 18,
      "ultima_semana": "2024-51"
    }
  }
}
```

## üìä Formato de Datos de Salida

Cada CSV generado contiene las siguientes columnas:

```
semana_a√±o                          # Identificador YYYY-WW
demanda_pacientes                   # Variable objetivo (a predecir)
demanda_lag1                        # Demanda de hace 1 semana
demanda_lag2                        # Demanda de hace 2 semanas
demanda_lag3                        # Demanda de hace 3 semanas
demanda_lag4                        # Demanda de hace 4 semanas
demanda_lag10                       # Demanda de hace 10 semanas
demanda_lag52                       # Demanda de hace 52 semanas (1 a√±o)
estancia (d√≠as)_lag1               # Estancia promedio (semana anterior)
tipo de paciente_No Qx_lag1        # Proporci√≥n no quir√∫rgicos
tipo de paciente_Qx_lag1           # Proporci√≥n quir√∫rgicos
tipo de ingreso_No Urgente_lag1    # Proporci√≥n no urgentes
tipo de ingreso_Urgente_lag1       # Proporci√≥n urgentes
estacion_invierno_lag1             # One-hot: invierno
estacion_oto√±o_lag1                # One-hot: oto√±o
estacion_primavera_lag1            # One-hot: primavera
estacion_verano_lag1               # One-hot: verano
numero_semana                       # N√∫mero de semana (1-52)
```

## üíæ Almacenamiento

El sistema soporta dos modos de almacenamiento:

### Modo Local (Desarrollo)

```bash
# Variables de entorno
STORAGE_TYPE=local
STORAGE_BASE_PATH=./data
```

Los archivos se guardan en el directorio `./data/`

### Modo S3 (Producci√≥n/Lambda)

```bash
# Variables de entorno
STORAGE_TYPE=s3
STORAGE_BASE_PATH=hospital-data
S3_BUCKET=mi-bucket-nombre
```

Los archivos se guardan en S3: `s3://mi-bucket-nombre/hospital-data/`

## üöÄ Uso

### 1. Instalar dependencias

```bash
cd backend
uv sync

# Para soporte S3 (opcional)
uv sync --extra aws
```

### 2. Iniciar servidor

```bash
cd backend
uv run uvicorn src.main:app --reload --host 0.0.0.0 --port 8000
```

### 3. Procesar Excel

```bash
curl -X POST http://localhost:8000/pipeline/process-excel \
  -F "file=@Datos_UC.xlsx"
```

O usar la interfaz Swagger: <http://localhost:8000/docs>

### 4. Verificar estado

```bash
curl http://localhost:8000/pipeline/status
```

### 5. Descargar CSVs

```bash
# Un archivo espec√≠fico
curl -O http://localhost:8000/pipeline/download/Alta

# Todos los archivos
curl -O http://localhost:8000/pipeline/download-all
```

## üîÑ Endpoints Existentes vs Nuevos

### Endpoints EXISTENTES (`/data/*`)

- `POST /data/send` - Recibir datos semanales (JSON)
- `POST /data/upload` - Subir Excel con datos semanales
- `GET /data/template` - Descargar plantilla

**Prop√≥sito:** Recibir datos semanales NUEVOS para hacer predicciones

### Endpoints NUEVOS (`/pipeline/*`)

- `POST /pipeline/process-excel` - Procesar Excel crudo inicial
- `GET /pipeline/download/{complejidad}` - Descargar CSV procesado
- `GET /pipeline/download-all` - Descargar todos los CSVs
- `GET /pipeline/status` - Ver estado del pipeline

**Prop√≥sito:** Limpieza inicial de datos hist√≥ricos para entrenar modelos

## üß™ Testing

### Test con curl

```bash
# 1. Subir y procesar Excel
curl -X POST http://localhost:8000/pipeline/process-excel \
  -H "Content-Type: multipart/form-data" \
  -F "file=@Datos_UC.xlsx"

# 2. Ver estado
curl http://localhost:8000/pipeline/status

# 3. Descargar CSV espec√≠fico
curl -O http://localhost:8000/pipeline/download/Alta

# 4. Descargar todo
curl -O http://localhost:8000/pipeline/download-all
```

### Test con Python

```python
import requests

# Procesar Excel
with open('Datos_UC.xlsx', 'rb') as f:
    response = requests.post(
        'http://localhost:8000/pipeline/process-excel',
        files={'file': f}
    )
    print(response.json())

# Ver estado
status = requests.get('http://localhost:8000/pipeline/status')
print(status.json())

# Descargar CSV
csv = requests.get('http://localhost:8000/pipeline/download/Alta')
with open('Alta.csv', 'wb') as f:
    f.write(csv.content)
```

## üì¶ Deployment en Lambda

### Configuraci√≥n

1. **Variables de entorno:**

```
STORAGE_TYPE=s3
STORAGE_BASE_PATH=hospital-data
S3_BUCKET=mi-bucket-nombre
```

2. **Permisos IAM:**

```json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": [
        "s3:GetObject",
        "s3:PutObject",
        "s3:HeadObject"
      ],
      "Resource": "arn:aws:s3:::mi-bucket-nombre/hospital-data/*"
    }
  ]
}
```

3. **Lambda Layer:**

- pandas
- numpy
- openpyxl
- boto3

### Consideraciones

- **Timeout:** Configurar al menos 5 minutos
- **Memoria:** M√≠nimo 512MB, recomendado 1GB
- **Tama√±o del Excel:** M√°ximo 50MB para Lambda
- **Procesamiento:** ~1-3 minutos por archivo t√≠pico

## üêõ Troubleshooting

### Error: "El archivo debe tener al menos 3 hojas"

**Soluci√≥n:** Verificar que el Excel tenga el formato correcto con 3+ hojas

### Error: "Datos insuficientes (< 55 semanas)"

**Soluci√≥n:** La complejidad no tiene suficientes datos hist√≥ricos, es normal

### Error: "File not found"

**Soluci√≥n:** Primero procesar el Excel con `/pipeline/process-excel`

### Error de importaci√≥n de pandas/numpy

**Soluci√≥n:** Ejecutar `uv sync` para instalar dependencias

## üìù Notas

- Las complejidades con menos de 55 semanas NO ser√°n procesadas
- Neonatolog√≠a no tiene filtro m√≠nimo de demanda (vs. otras que requieren ‚â•10 pacientes/semana)
- Los lags se crean autom√°ticamente: 1, 2, 3, 4, 10, 52 semanas
- Las columnas de servicios poco frecuentes se eliminan autom√°ticamente

## üîó Referencias

- Notebook original: `Limpieza.ipynb`
- FastAPI docs: <http://localhost:8000/docs>
- ReDoc: <http://localhost:8000/redoc>
